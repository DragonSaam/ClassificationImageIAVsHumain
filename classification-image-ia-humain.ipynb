{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91198,"databundleVersionId":10884264,"sourceType":"competition"},{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Import Bibliotheques - Import Library\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Importation des fichiers csv et création du chemin de récupération des images\n#Import CSV files and create recuperation path for images\n\nfolder_path = \"/kaggle/input/ai-vs-human-generated-dataset\"\ntrain_csv_path = '/kaggle/input/ai-vs-human-generated-dataset/train.csv'\ntrain_csv = pd.read_csv(train_csv_path)\n\ndef get_image_path(folder_path, image_name) :\n    return os.path.join(folder_path, image_name)\n\n#Ajout du chemin complet de l'image dans file_name\n#Add complet path of the image in the colomns file_name\ntrain_csv['file_name'] = train_csv['file_name'].apply(lambda name : get_image_path(folder_path, name))\n\n\nprint(train_csv.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Chargement d'un Dataset léger pour tester mon code\n#Low weight dataset loding - in order to test my code before the big loading\ntrain_test_csv = train_csv.head(1000)\ntrain_data_set = train_test_csv\n\n#Chargement du Dataset complet\n#Full dataset loading\ntrain_data_set = train_csv\n\ndf = pd.DataFrame(train_data_set)\n\n#Dataset splité\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Définir des transformations\n#Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((380, 380)),  # Redimensionner toutes les images\n    #transforms.RandomRotation(15),  # Rotation aléatoire jusqu'à 15°\n    #transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Recadrage aléatoire\n    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Variations de couleur\n    transforms.ToTensor(),       # Convertir en tenseur PyTorch\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalisation ImageNet\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Création d'un DataLoader avec une classe\n#Dataloader own class creation\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['file_name']\n        label = self.dataframe.iloc[idx]['label']\n\n        # Charger l'image\n        # load image\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Appliquer les transformations si spécifiées\n        # Apply transformation\n        if self.transform:\n            image = self.transform(image)\n\n        return image, torch.tensor(label, dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Création d'une instance de DataSet customisé\n#Dataset with transformation creation\n\n#custom_dataset = CustomDataset(df, transform=transform)\n#Dataset splité\ntrain_dataset = CustomDataset(train_df, transform=transform)\nval_dataset = CustomDataset(val_df, transform=transform)\ntest_dataset = CustomDataset(test_df, transform=transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Création d'un Dataloader pour itération des données\n#Dataloader for data iteration\n\n#custom_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n#Dataset splité\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Charger le modèle EfficientNet-B0 pré-entraîné\n#model = models.efficientnet_b0(pretrained=True)\n\n# Charger MobileNetV2 pré-entraîné\n#model = models.mobilenet_v2(pretrained=True)\n\n# Adapter la dernière couche pour une classification binaire\n#model.classifier[1] = nn.Sequential(\n#    nn.Linear(model.last_channel, 1),  # Une seule sortie pour la classification binaire\n#    nn.Sigmoid()  # Activation pour les probabilités\n#)\n\n# Charger le modèle ResNet-18 pré-entraîné\n#model = models.resnet18(pretrained=True)\n# Adapter la dernière couche pour une sortie binaire\n#num_features = model.fc.in_features\n#model.fc = nn.Sequential(\n#    nn.Linear(num_features, 1),\n#    nn.Sigmoid()\n#)\n\n# Charger SqueezeNet pré-entraîné\n#model = models.squeezenet1_1(pretrained=True)\n# Modifier la dernière couche du modèle pour SqueezeNet\n#model.classifier[1] = nn.Conv2d(512, 1, kernel_size=(1, 1))\n\n# Charger le modèle EfficientNet-B4 pré-entraîné\nmodel = models.efficientnet_b4(pretrained=True)\n# Adapter la dernière couche pour une sortie binaire\nnum_features = model.classifier[1].in_features\nmodel.classifier = nn.Sequential(\n    nn.Linear(num_features, 1),  # Une seule sortie pour binaire\n    nn.Sigmoid()  # Activation sigmoïde pour probabilité\n)\n\n\n# Déplacer le modèle sur le GPU si disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n#print(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Définir la fonction de perte et l'optimiseur\n#Define lost function and optimiser\ncriterion = nn.BCELoss()  # Binary Cross-Entropy Loss\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Entraînement du modèle 1\n#Fit model 1\n# Entraînement du modèle\nfor epoch in range(10):  # Nombre d'époques\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        labels = labels.view(-1, 1)  # Adapter les dimensions des labels\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass et optimisation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    # Évaluation sur l'ensemble de validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            labels = labels.view(-1, 1)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            predictions = (outputs > 0.5).float()\n            correct += (predictions.view(-1) == labels.view(-1)).sum().item()\n            total += labels.size(0)\n\n    val_accuracy = 100 * correct / total\n    print(f\"Époque {epoch+1}, Perte entraînement : {running_loss/len(train_loader):.4f}, Perte validation : {val_loss/len(val_loader):.4f}, Exactitude validation : {val_accuracy:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Évaluation du modèle\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        predictions = (outputs > 0.5).float()\n        correct += (predictions.view(-1) == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Exactitude : {100 * correct / total:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sauvegarder le modèle\ntorch.save(model.state_dict(), 'efficientnet_b4_model.pth')\n\n# Charger le modèle pour l'inférence\nmodel.load_state_dict(torch.load('efficientnet_b4_model.pth'))\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CLASSIFICATION DES IMAGES TEST NON ETIQUETEES - UNLABELED IMAGES CLASSIFICATION**","metadata":{}},{"cell_type":"code","source":"#Préparation des données de test final\n#Final test data preparation\ntest_csv_path = '/kaggle/input/ai-vs-human-generated-dataset/test.csv'\ntest_csv = pd.read_csv(test_csv_path)\nid_csv = test_csv['id']\ntest_csv['id'] = test_csv['id'].apply(lambda name : get_image_path(folder_path, name))\n\n#Chargement d'un Dataset léger pour tester mon code\n#Low weight dataset loding - in order to test my code before the big loading\ntest_test_csv = test_csv.head(50)\ntest_data_set = test_test_csv\n\n#Chargement du Dataset complet\n#Full dataset loading\ntest_data_set = test_csv\n\n#Création du DataFrame de test\n#Test Dataframe creation\ndf_test = pd.DataFrame(test_data_set)\nprint(id_csv.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Création d'un DataLoader_test avec une classe\n#Dataloader own class creation\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['id']\n\n        # Charger l'image\n        # load image\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Appliquer les transformations si spécifiées\n        # Apply transformation\n        if self.transform:\n            image = self.transform(image)\n\n        return image, self.dataframe.iloc[idx]['id']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Création d'une instance de DataSet customisé\n#Dataset with transformation creation\ncustom_dataset_test = CustomDataset(df_test, transform=transform)\n\n#Création d'un Dataloader pour itération des données\n#Dataloader for data iteration\ncustom_loader_test = DataLoader(custom_dataset_test, batch_size=32, shuffle=True, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Liste pour stocker les résultats\nresults = []\nid_counter = 0  # Initialisation du compteur d'ID","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Itérer sur le DataLoader et faire des prédictions\n# Prédictions sur les données non étiquetées\nmodel.eval()\nwith torch.no_grad():\n    for i, (inputs, _) in enumerate(custom_loader_test):  # On ignore le label puisque les images sont non étiquetées\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        predictions = (outputs > 0.5).int()  # Convertir les probabilités en 0/1\n\n        # Ajouter chaque prédiction au tableau des résultats\n        for pred in predictions.cpu().numpy():\n            #results.append({'id': len(results) + 1, 'label': int(pred)})\n            results.append({'id': id_csv[id_counter], 'label': int(pred.item() if hasattr(pred, 'item') else pred)})\n            id_counter += 1  # Incrémenter le compteur\n\n\n# Convertir en DataFrame\nresults_df = pd.DataFrame(results)\n\n# Sauvegarder le DataFrame en fichier CSV\noutput_csv_path = \"predictions.csv\"\n#results_df.to_csv(output_csv_path, index=False, sep=';')\nresults_df.to_csv(output_csv_path, index=False, sep=',')\n\nprint(f\"Fichier CSV généré : {output_csv_path}\")\nprint(results_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(output_csv_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}