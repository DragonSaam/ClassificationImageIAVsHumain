{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562eb997",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:23.529721Z",
     "iopub.status.busy": "2025-03-02T15:09:23.529418Z",
     "iopub.status.idle": "2025-03-02T15:09:24.282174Z",
     "shell.execute_reply": "2025-03-02T15:09:24.281158Z"
    },
    "papermill": {
     "duration": 0.760108,
     "end_time": "2025-03-02T15:09:24.283878",
     "exception": false,
     "start_time": "2025-03-02T15:09:23.523770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ef01e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:24.293093Z",
     "iopub.status.busy": "2025-03-02T15:09:24.292712Z",
     "iopub.status.idle": "2025-03-02T15:09:33.004980Z",
     "shell.execute_reply": "2025-03-02T15:09:33.003970Z"
    },
    "papermill": {
     "duration": 8.718674,
     "end_time": "2025-03-02T15:09:33.006882",
     "exception": false,
     "start_time": "2025-03-02T15:09:24.288208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import Bibliotheques - Import Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83017cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:33.016265Z",
     "iopub.status.busy": "2025-03-02T15:09:33.015811Z",
     "iopub.status.idle": "2025-03-02T15:09:33.283274Z",
     "shell.execute_reply": "2025-03-02T15:09:33.282290Z"
    },
    "papermill": {
     "duration": 0.273768,
     "end_time": "2025-03-02T15:09:33.284957",
     "exception": false,
     "start_time": "2025-03-02T15:09:33.011189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                          file_name  label\n",
      "0           0  /kaggle/input/ai-vs-human-generated-dataset/tr...      1\n",
      "1           1  /kaggle/input/ai-vs-human-generated-dataset/tr...      0\n",
      "2           2  /kaggle/input/ai-vs-human-generated-dataset/tr...      1\n",
      "3           3  /kaggle/input/ai-vs-human-generated-dataset/tr...      0\n",
      "4           4  /kaggle/input/ai-vs-human-generated-dataset/tr...      1\n"
     ]
    }
   ],
   "source": [
    "#Importation des fichiers csv et création du chemin de récupération des images\n",
    "#Import CSV files and create recuperation path for images\n",
    "\n",
    "folder_path = \"/kaggle/input/ai-vs-human-generated-dataset\"\n",
    "train_csv_path = '/kaggle/input/ai-vs-human-generated-dataset/train.csv'\n",
    "train_csv = pd.read_csv(train_csv_path)\n",
    "\n",
    "def get_image_path(folder_path, image_name) :\n",
    "    return os.path.join(folder_path, image_name)\n",
    "\n",
    "#Ajout du chemin complet de l'image dans file_name\n",
    "#Add complet path of the image in the colomns file_name\n",
    "train_csv['file_name'] = train_csv['file_name'].apply(lambda name : get_image_path(folder_path, name))\n",
    "\n",
    "\n",
    "print(train_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd23c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:33.294366Z",
     "iopub.status.busy": "2025-03-02T15:09:33.294098Z",
     "iopub.status.idle": "2025-03-02T15:09:33.313587Z",
     "shell.execute_reply": "2025-03-02T15:09:33.312879Z"
    },
    "papermill": {
     "duration": 0.02578,
     "end_time": "2025-03-02T15:09:33.315167",
     "exception": false,
     "start_time": "2025-03-02T15:09:33.289387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Chargement d'un Dataset léger pour tester mon code\n",
    "#Low weight dataset loding - in order to test my code before the big loading\n",
    "train_test_csv = train_csv.head(1000)\n",
    "train_data_set = train_test_csv\n",
    "\n",
    "#Chargement du Dataset complet\n",
    "#Full dataset loading\n",
    "train_data_set = train_csv\n",
    "\n",
    "df = pd.DataFrame(train_data_set)\n",
    "\n",
    "#Dataset splité\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2806da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:33.324762Z",
     "iopub.status.busy": "2025-03-02T15:09:33.324512Z",
     "iopub.status.idle": "2025-03-02T15:09:33.328624Z",
     "shell.execute_reply": "2025-03-02T15:09:33.327979Z"
    },
    "papermill": {
     "duration": 0.010103,
     "end_time": "2025-03-02T15:09:33.329879",
     "exception": false,
     "start_time": "2025-03-02T15:09:33.319776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définir des transformations\n",
    "#Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),  # Taille requise pour ConvNeXt\n",
    "    transforms.RandomHorizontalFlip(),  # Rotation aléatoire jusqu'à 15°\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),       # Convertir en tenseur PyTorch\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalisation ImageNet\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b92686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:33.339435Z",
     "iopub.status.busy": "2025-03-02T15:09:33.339154Z",
     "iopub.status.idle": "2025-03-02T15:09:33.344137Z",
     "shell.execute_reply": "2025-03-02T15:09:33.343410Z"
    },
    "papermill": {
     "duration": 0.011135,
     "end_time": "2025-03-02T15:09:33.345353",
     "exception": false,
     "start_time": "2025-03-02T15:09:33.334218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Création d'un DataLoader avec une classe\n",
    "#Dataloader own class creation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['file_name']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "\n",
    "        # Charger l'image\n",
    "        # load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Appliquer les transformations si spécifiées\n",
    "        # Apply transformation\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaac869f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:33.354200Z",
     "iopub.status.busy": "2025-03-02T15:09:33.353972Z",
     "iopub.status.idle": "2025-03-02T15:09:33.357608Z",
     "shell.execute_reply": "2025-03-02T15:09:33.356850Z"
    },
    "papermill": {
     "duration": 0.009291,
     "end_time": "2025-03-02T15:09:33.358867",
     "exception": false,
     "start_time": "2025-03-02T15:09:33.349576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Création d'une instance de DataSet customisé\n",
    "#Dataset with transformation creation\n",
    "\n",
    "#custom_dataset = CustomDataset(df, transform=transform)\n",
    "#Dataset splité\n",
    "train_dataset = CustomDataset(train_df, transform=transform)\n",
    "val_dataset = CustomDataset(val_df, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db3b3e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:33.367919Z",
     "iopub.status.busy": "2025-03-02T15:09:33.367624Z",
     "iopub.status.idle": "2025-03-02T15:09:33.371598Z",
     "shell.execute_reply": "2025-03-02T15:09:33.370930Z"
    },
    "papermill": {
     "duration": 0.009975,
     "end_time": "2025-03-02T15:09:33.372933",
     "exception": false,
     "start_time": "2025-03-02T15:09:33.362958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Création d'un Dataloader pour itération des données\n",
    "#Dataloader for data iteration\n",
    "\n",
    "#custom_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n",
    "#Dataset splité\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5a5bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:33.382280Z",
     "iopub.status.busy": "2025-03-02T15:09:33.381991Z",
     "iopub.status.idle": "2025-03-02T15:09:36.985132Z",
     "shell.execute_reply": "2025-03-02T15:09:36.984404Z"
    },
    "papermill": {
     "duration": 3.609708,
     "end_time": "2025-03-02T15:09:36.986845",
     "exception": false,
     "start_time": "2025-03-02T15:09:33.377137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base-6075fbad.pth\n",
      "100%|██████████| 338M/338M [00:01<00:00, 227MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle ConvNeXt pré-entraîné d'origine\n",
    "model = models.convnext_base(weights=\"DEFAULT\")\n",
    "# Adapter la dernière couche pour une sortie binaire\n",
    "num_features = model.classifier[2].in_features\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.Linear(num_features, 1),  # Une seule sortie pour binaire\n",
    "    nn.Sigmoid()  # Activation sigmoïde pour probabilité\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87cc1dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:36.998804Z",
     "iopub.status.busy": "2025-03-02T15:09:36.998446Z",
     "iopub.status.idle": "2025-03-02T15:09:37.002035Z",
     "shell.execute_reply": "2025-03-02T15:09:37.001147Z"
    },
    "papermill": {
     "duration": 0.011006,
     "end_time": "2025-03-02T15:09:37.003515",
     "exception": false,
     "start_time": "2025-03-02T15:09:36.992509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#print(os.listdir(\"/kaggle/input/convnext_model_sys/pytorch/default/1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "313c28a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:37.014205Z",
     "iopub.status.busy": "2025-03-02T15:09:37.013977Z",
     "iopub.status.idle": "2025-03-02T15:09:40.247496Z",
     "shell.execute_reply": "2025-03-02T15:09:40.246593Z"
    },
    "papermill": {
     "duration": 3.240449,
     "end_time": "2025-03-02T15:09:40.248995",
     "exception": false,
     "start_time": "2025-03-02T15:09:37.008546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-7356e10fb697>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/kaggle/input/convnext_model_sys/pytorch/default/1/convnext_model_SYS.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.014285714285714285, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.02857142857142857, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04285714285714286, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05714285714285714, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07142857142857142, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08571428571428572, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11428571428571428, mode=row)\n",
       "      )\n",
       "      (3): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12857142857142856, mode=row)\n",
       "      )\n",
       "      (4): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14285714285714285, mode=row)\n",
       "      )\n",
       "      (5): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15714285714285714, mode=row)\n",
       "      )\n",
       "      (6): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17142857142857143, mode=row)\n",
       "      )\n",
       "      (7): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18571428571428572, mode=row)\n",
       "      )\n",
       "      (8): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "      )\n",
       "      (9): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.21428571428571427, mode=row)\n",
       "      )\n",
       "      (10): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.22857142857142856, mode=row)\n",
       "      )\n",
       "      (11): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.24285714285714285, mode=row)\n",
       "      )\n",
       "      (12): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2571428571428571, mode=row)\n",
       "      )\n",
       "      (13): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2714285714285714, mode=row)\n",
       "      )\n",
       "      (14): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2857142857142857, mode=row)\n",
       "      )\n",
       "      (15): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.3, mode=row)\n",
       "      )\n",
       "      (16): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.3142857142857143, mode=row)\n",
       "      )\n",
       "      (17): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.32857142857142857, mode=row)\n",
       "      )\n",
       "      (18): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.34285714285714286, mode=row)\n",
       "      )\n",
       "      (19): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.35714285714285715, mode=row)\n",
       "      )\n",
       "      (20): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.37142857142857144, mode=row)\n",
       "      )\n",
       "      (21): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.38571428571428573, mode=row)\n",
       "      )\n",
       "      (22): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.4, mode=row)\n",
       "      )\n",
       "      (23): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.4142857142857143, mode=row)\n",
       "      )\n",
       "      (24): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.42857142857142855, mode=row)\n",
       "      )\n",
       "      (25): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.44285714285714284, mode=row)\n",
       "      )\n",
       "      (26): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.45714285714285713, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.4714285714285714, mode=row)\n",
       "      )\n",
       "      (1): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.4857142857142857, mode=row)\n",
       "      )\n",
       "      (2): CNBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "          (1): Permute()\n",
       "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (4): GELU(approximate='none')\n",
       "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (6): Permute()\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger le modèle sauvegardé\n",
    "import kagglehub\n",
    "#convnext_model_sys_pytorch_default_1_path = kagglehub.model_download('/kaggle/input/convnext_model_sys/pytorch/default/1/convnext_model_SYS.pth')\n",
    "\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/convnext_model_sys/pytorch/default/1/convnext_model_SYS.pth\"))\n",
    "model.train()  # Remettre le modèle en mode entraînement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d429ba56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:40.261908Z",
     "iopub.status.busy": "2025-03-02T15:09:40.261629Z",
     "iopub.status.idle": "2025-03-02T15:09:40.609222Z",
     "shell.execute_reply": "2025-03-02T15:09:40.608261Z"
    },
    "papermill": {
     "duration": 0.355512,
     "end_time": "2025-03-02T15:09:40.610942",
     "exception": false,
     "start_time": "2025-03-02T15:09:40.255430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Déplacer le modèle sur le GPU si disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1bd586c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:40.623696Z",
     "iopub.status.busy": "2025-03-02T15:09:40.623418Z",
     "iopub.status.idle": "2025-03-02T15:09:40.628409Z",
     "shell.execute_reply": "2025-03-02T15:09:40.627669Z"
    },
    "papermill": {
     "duration": 0.012928,
     "end_time": "2025-03-02T15:09:40.629785",
     "exception": false,
     "start_time": "2025-03-02T15:09:40.616857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Définir la fonction de perte et l'optimiseur\n",
    "#Define lost function and optimiser\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9de840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T15:09:40.642459Z",
     "iopub.status.busy": "2025-03-02T15:09:40.642151Z",
     "iopub.status.idle": "2025-03-02T20:49:54.130476Z",
     "shell.execute_reply": "2025-03-02T20:49:54.129582Z"
    },
    "papermill": {
     "duration": 20413.502862,
     "end_time": "2025-03-02T20:49:54.138642",
     "exception": false,
     "start_time": "2025-03-02T15:09:40.635780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 1, Perte entraînement : 0.0969, Perte validation : 0.0885, Exactitude validation : 96.69%\n",
      "Époque 2, Perte entraînement : 0.0767, Perte validation : 0.0690, Exactitude validation : 97.54%\n"
     ]
    }
   ],
   "source": [
    "#Entraînement du modèle\n",
    "#Fit model\n",
    "# Entraînement du modèle\n",
    "for epoch in range(2):  # Nombre d'époques\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1, 1)  # Adapter les dimensions des labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass et optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Évaluation sur l'ensemble de validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions.view(-1) == labels.view(-1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Époque {epoch+1}, Perte entraînement : {running_loss/len(train_loader):.4f}, Perte validation : {val_loss/len(val_loader):.4f}, Exactitude validation : {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1856b4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:49:54.150963Z",
     "iopub.status.busy": "2025-03-02T20:49:54.150714Z",
     "iopub.status.idle": "2025-03-02T20:54:43.869844Z",
     "shell.execute_reply": "2025-03-02T20:54:43.868691Z"
    },
    "papermill": {
     "duration": 289.732429,
     "end_time": "2025-03-02T20:54:43.876964",
     "exception": false,
     "start_time": "2025-03-02T20:49:54.144535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude : 97.40%\n"
     ]
    }
   ],
   "source": [
    "# Évaluation du modèle\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        correct += (predictions.view(-1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Exactitude : {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24073db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:54:43.890404Z",
     "iopub.status.busy": "2025-03-02T20:54:43.889968Z",
     "iopub.status.idle": "2025-03-02T20:54:44.376183Z",
     "shell.execute_reply": "2025-03-02T20:54:44.375358Z"
    },
    "papermill": {
     "duration": 0.49506,
     "end_time": "2025-03-02T20:54:44.377857",
     "exception": false,
     "start_time": "2025-03-02T20:54:43.882797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), 'convnext_model_SYS_V3_Epoque4_lr00005.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99edf634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:54:44.391079Z",
     "iopub.status.busy": "2025-03-02T20:54:44.390813Z",
     "iopub.status.idle": "2025-03-02T20:54:44.393919Z",
     "shell.execute_reply": "2025-03-02T20:54:44.393167Z"
    },
    "papermill": {
     "duration": 0.011148,
     "end_time": "2025-03-02T20:54:44.395189",
     "exception": false,
     "start_time": "2025-03-02T20:54:44.384041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Charger le modèle pour l'inférence\n",
    "#model.load_state_dict(torch.load('convnext_model_SYS.pth', map_location=device))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02a1b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.005422,
     "end_time": "2025-03-02T20:54:44.406449",
     "exception": false,
     "start_time": "2025-03-02T20:54:44.401027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**CLASSIFICATION DES IMAGES TEST NON ETIQUETEES - UNLABELED IMAGES CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ff4e8ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:54:44.418838Z",
     "iopub.status.busy": "2025-03-02T20:54:44.418520Z",
     "iopub.status.idle": "2025-03-02T20:54:44.456164Z",
     "shell.execute_reply": "2025-03-02T20:54:44.455014Z"
    },
    "papermill": {
     "duration": 0.04587,
     "end_time": "2025-03-02T20:54:44.457849",
     "exception": false,
     "start_time": "2025-03-02T20:54:44.411979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    test_data_v2/1a2d9fd3e21b4266aea1f66b30aed157.jpg\n",
      "1    test_data_v2/ab5df8f441fe4fbf9dc9c6baae699dc7.jpg\n",
      "2    test_data_v2/eb364dd2dfe34feda0e52466b7ce7956.jpg\n",
      "3    test_data_v2/f76c2580e9644d85a741a42c6f6b39c0.jpg\n",
      "4    test_data_v2/a16495c578b7494683805484ca27cf9f.jpg\n",
      "Name: id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Préparation des données de test final\n",
    "#Final test data preparation\n",
    "test_csv_path = '/kaggle/input/ai-vs-human-generated-dataset/test.csv'\n",
    "test_csv = pd.read_csv(test_csv_path)\n",
    "id_csv = test_csv['id']\n",
    "test_csv['id'] = test_csv['id'].apply(lambda name : get_image_path(folder_path, name))\n",
    "\n",
    "#Chargement d'un Dataset léger pour tester mon code\n",
    "#Low weight dataset loding - in order to test my code before the big loading\n",
    "test_test_csv = test_csv.head(50)\n",
    "test_data_set = test_test_csv\n",
    "\n",
    "#Chargement du Dataset complet\n",
    "#Full dataset loading\n",
    "test_data_set = test_csv\n",
    "\n",
    "#Création du DataFrame de test\n",
    "#Test Dataframe creation\n",
    "df_test = pd.DataFrame(test_data_set)\n",
    "print(id_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ce58233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:54:44.476225Z",
     "iopub.status.busy": "2025-03-02T20:54:44.475950Z",
     "iopub.status.idle": "2025-03-02T20:54:44.480840Z",
     "shell.execute_reply": "2025-03-02T20:54:44.480068Z"
    },
    "papermill": {
     "duration": 0.013027,
     "end_time": "2025-03-02T20:54:44.482157",
     "exception": false,
     "start_time": "2025-03-02T20:54:44.469130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Création d'un DataLoader_test avec une classe\n",
    "#Dataloader own class creation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['id']\n",
    "\n",
    "        # Charger l'image\n",
    "        # load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Appliquer les transformations si spécifiées\n",
    "        # Apply transformation\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.dataframe.iloc[idx]['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3cb111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:54:44.495039Z",
     "iopub.status.busy": "2025-03-02T20:54:44.494670Z",
     "iopub.status.idle": "2025-03-02T20:54:44.499507Z",
     "shell.execute_reply": "2025-03-02T20:54:44.498641Z"
    },
    "papermill": {
     "duration": 0.012702,
     "end_time": "2025-03-02T20:54:44.500787",
     "exception": false,
     "start_time": "2025-03-02T20:54:44.488085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Création d'une instance de DataSet customisé\n",
    "#Dataset with transformation creation\n",
    "custom_dataset_test = CustomDataset(df_test, transform=transform)\n",
    "\n",
    "#Création d'un Dataloader pour itération des données\n",
    "#Dataloader for data iteration\n",
    "custom_loader_test = DataLoader(custom_dataset_test, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dac02d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:54:44.513665Z",
     "iopub.status.busy": "2025-03-02T20:54:44.513303Z",
     "iopub.status.idle": "2025-03-02T20:54:44.517865Z",
     "shell.execute_reply": "2025-03-02T20:54:44.517120Z"
    },
    "papermill": {
     "duration": 0.012461,
     "end_time": "2025-03-02T20:54:44.519153",
     "exception": false,
     "start_time": "2025-03-02T20:54:44.506692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Liste pour stocker les résultats\n",
    "results = []\n",
    "id_counter = 0  # Initialisation du compteur d'ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b96084ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:54:44.532244Z",
     "iopub.status.busy": "2025-03-02T20:54:44.531973Z",
     "iopub.status.idle": "2025-03-02T20:56:40.785551Z",
     "shell.execute_reply": "2025-03-02T20:56:40.784500Z"
    },
    "papermill": {
     "duration": 116.266787,
     "end_time": "2025-03-02T20:56:40.791934",
     "exception": false,
     "start_time": "2025-03-02T20:54:44.525147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV généré : predictions.csv\n",
      "                                                     id  label\n",
      "0     test_data_v2/1a2d9fd3e21b4266aea1f66b30aed157.jpg      0\n",
      "1     test_data_v2/ab5df8f441fe4fbf9dc9c6baae699dc7.jpg      1\n",
      "2     test_data_v2/eb364dd2dfe34feda0e52466b7ce7956.jpg      1\n",
      "3     test_data_v2/f76c2580e9644d85a741a42c6f6b39c0.jpg      0\n",
      "4     test_data_v2/a16495c578b7494683805484ca27cf9f.jpg      0\n",
      "...                                                 ...    ...\n",
      "5535  test_data_v2/483412064ff74d9d9472d606b65976d9.jpg      0\n",
      "5536  test_data_v2/c0b49ba4081a4197b422dac7c15aea7f.jpg      0\n",
      "5537  test_data_v2/01454aaedec140c0a3ca1f48028c41cf.jpg      0\n",
      "5538  test_data_v2/e9adfea8b67e4791968c4c2bdd8ec343.jpg      0\n",
      "5539  test_data_v2/ba8f4198e8d74d3394fa56c56af23442.jpg      0\n",
      "\n",
      "[5540 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Itérer sur le DataLoader et faire des prédictions\n",
    "# Prédictions sur les données non étiquetées\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, _) in enumerate(custom_loader_test):  # On ignore le label puisque les images sont non étiquetées\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs > 0.5).int()  # Convertir les probabilités en 0/1\n",
    "\n",
    "        # Ajouter chaque prédiction au tableau des résultats\n",
    "        for pred in predictions.cpu().numpy():\n",
    "            #results.append({'id': len(results) + 1, 'label': int(pred)})\n",
    "            results.append({'id': id_csv[id_counter], 'label': int(pred.item() if hasattr(pred, 'item') else pred)})\n",
    "            id_counter += 1  # Incrémenter le compteur\n",
    "\n",
    "\n",
    "# Convertir en DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sauvegarder le DataFrame en fichier CSV\n",
    "output_csv_path = \"predictions.csv\"\n",
    "#results_df.to_csv(output_csv_path, index=False, sep=';')\n",
    "results_df.to_csv(output_csv_path, index=False, sep=',')\n",
    "\n",
    "print(f\"Fichier CSV généré : {output_csv_path}\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eb76d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T20:56:40.804841Z",
     "iopub.status.busy": "2025-03-02T20:56:40.804558Z",
     "iopub.status.idle": "2025-03-02T20:56:40.809651Z",
     "shell.execute_reply": "2025-03-02T20:56:40.808952Z"
    },
    "papermill": {
     "duration": 0.012961,
     "end_time": "2025-03-02T20:56:40.810856",
     "exception": false,
     "start_time": "2025-03-02T20:56:40.797895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='predictions.csv' target='_blank'>predictions.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/predictions.csv"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10884264,
     "sourceId": 91198,
     "sourceType": "competition"
    },
    {
     "datasetId": 6412205,
     "sourceId": 10550636,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 252710,
     "modelInstanceId": 230953,
     "sourceId": 269877,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 253828,
     "modelInstanceId": 232096,
     "sourceId": 271149,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20842.007141,
   "end_time": "2025-03-02T20:56:42.861111",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-02T15:09:20.853970",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
